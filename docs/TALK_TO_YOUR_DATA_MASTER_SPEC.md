# Neural Analyst â€” â€œTalk to Your Dataâ€ Documentation Pack (Master Spec + Gap Analysis)

**Date:** 2026â€‘02â€‘03  
**Repository:** `Laxmi-Narasimha/neural-analyst`  
**Audience:** Product owner, engineering, data science, security, QA  
**Constraint:** This document contains **no code snippets**; it specifies behavior, features, and implementation requirements.

This master spec is the entry point to a **30â€“40 page documentation pack** for building a world-class â€œAI Data Analyst / AI Data Scientistâ€ experience that works primarily via prompts and guided UI (not by users writing code).

**Documentation pack (read in order):**
1. `docs/TALK_TO_YOUR_DATA_MASTER_SPEC.md` (this file) â€” vision, ground truth audit, feature coverage matrix, top-level roadmap.
2. `docs/TALK_TO_YOUR_DATA_FEATURE_SPEC.md` â€” detailed product requirements and UX flows for every major feature.
3. `docs/TALK_TO_YOUR_DATA_ENGINEERING_SPEC.md` â€” detailed architecture: dataset lifecycle, compute layer, agent tools, job system, persistence.
4. `docs/TALK_TO_YOUR_DATA_SECURITY_EVAL_SPEC.md` â€” threat model, safety constraints, evaluation harness, quality gates.
5. `docs/TALK_TO_YOUR_DATA_OPERATOR_CATALOG.md` â€” allow-listed compute primitives (operators) that make the assistant â€œdynamicâ€ without unsafe code execution.
6. `docs/TALK_TO_YOUR_DATA_BACKLOG.md` â€” prioritized implementation backlog mapped to this repoâ€™s modules (P0/P1/P2), with acceptance criteria.

---

## 0) Executive Summary

### 0.1 Your intended product (what â€œgoodâ€ looks like)
The product should make analysts feel like they can:
- upload or connect data,
- press a single â€œTalk to your data / Make the data speakâ€ button,
- immediately receive a truthful, evidence-backed story of what the data contains,
- then ask follow-up questions in natural language and receive grounded computations, charts, and reproducible steps.

The goal is not to replace analysts; it is to make them drastically faster and more effective.

### 0.2 What exists today (truth based on the repo)
The repo currently provides:
- A polished Next.js UI with a chat-like analysis page and dataset pages.
- A FastAPI backend with structured logging, configuration scaffolding, and SQLAlchemy models.
- Many analytics/ML â€œengineâ€ modules under `ai-data-analyst/backend/app/ml/*`.
- A separate â€œdata adequacyâ€ validator (FastAPI + Streamlit), plus an embedded copy of its â€œdata adequacyâ€ manager under `ai-data-analyst/backend/app/agents/data_adequacy/*`.

### 0.3 The critical gap
The current system does **not** yet reliably execute analysis on the userâ€™s actual uploaded datasets end-to-end. As a result:
- â€œchatâ€ can produce plausible text without computed evidence,
- analytics endpoints often run on **mock data**,
- the â€œorchestratorâ€ agent contains multiple **placeholder tool implementations**,
- several UI pages fall back to hard-coded example datasets.

This is not a â€œsmall missing featureâ€; it is the central product promise. The fastest way to unlock the wow factor is to build a real **dataset â†’ compute â†’ evidence â†’ narrative** loop, then route all conversational answers through that loop.

### 0.4 The non-negotiable design choice
To meet your â€œdynamic, no hard-coded fields, no ambiguitiesâ€ goal without compromising safety:
- Do **not** rely on arbitrary LLM-generated Python being executed.
- Instead, have the LLM generate a **constrained analysis plan** and execute it with a safe compute layer (SQL/DuckDB + approved operators).

This architecture can still feel â€œunlimitedâ€ to the user while remaining reliable and secure.

---

## 1) Product Principles (What makes the app â€œmesmerizingâ€)

### 1.1 Evidence-first, not vibes-first
Every insight must come with:
- a computed value (table/metric),
- the slice/filters used,
- and a short explanation of â€œwhy this mattersâ€.

If the system cannot compute the answer, it must say so and propose the next best step (sampling, data selection, or a clarification question).

### 1.2 Dynamic, but deterministic
The system must infer:
- likely target columns,
- time columns,
- IDs and entity keys,
- categorical vs text fields,
- and risk flags (PII, leakage, drift indicators),
but do so in a reproducible way (heuristics + scoring + explainable confidence).

### 1.3 The assistant is a workflow, not a chatbot
The â€œTalk to your dataâ€ feature is not just LLM text. It is:
- a guided workflow,
- with tool calls and computation,
- with visual outputs,
- with saved artifacts and reproducibility.

### 1.4 Safe by default
The assistant must not be tricked by:
- prompt injection embedded inside dataset text,
- attempts to exfiltrate secrets,
- attempts to execute system commands,
- or adversarially-crafted data.

### 1.5 Performance-aware
For large datasets:
- compute should start with sampling and summaries,
- then progressively refine results,
- and offload heavy tasks to background jobs.

---

## 2) Ground Truth Audit (Specific shortcomings today)

This section highlights the most important gaps discovered in the current repo. It is not exhaustive; the detailed backlog lives in the feature/engineering docs.

### 2.1 â€œTalk to your dataâ€ is not grounded end-to-end
Key symptoms:
- Chat responses can be generated without querying dataset contents.
- Analytics endpoints can operate on generated mock data rather than real uploaded data.

Where it shows up:
- The analytics routes include a placeholder dataset loader that generates sample data instead of loading by `dataset_id`.
- The orchestratorâ€™s internal tool methods return placeholder structures for summary/statistics/modeling.

### 2.2 API contract mismatches between frontend and backend
Examples:
- Dataset upload response fields expected by UI donâ€™t match backend envelopes.
- Analysis button endpoints differ between UI and backend.
- Multiple pages rely on mock/fallback data due to contract errors.

### 2.3 User identity and persistence are incomplete
Symptoms:
- Routes use random user IDs in multiple places, so datasets and conversations donâ€™t reliably show up for the â€œsame userâ€.
- In-memory stores are used where persistence is required (sessions, connections).

### 2.4 Background execution and results persistence are incomplete
Symptoms:
- Analyses are queued, but results are not persisted in the DB in a consistent â€œanalysis resultâ€ model.
- Heavy tasks run inline or are stubbed; no durable queue/worker model exists.

### 2.5 File lifecycle inconsistencies
Symptoms:
- Some flows refer to file paths that are never actually uploaded.
- Runtime uploads are stored locally without a unified abstraction, making scaling or multi-instance deployments hard.

---

## 3) Feature Coverage Matrix (What you have vs what you need)

Legend:
- âœ… Implemented (usable end-to-end)
- ğŸŸ¡ Partial (exists but not integrated or lacks key pieces)
- ğŸ§© Stub/placeholder (returns fake results or doesnâ€™t use real data)
- âŒ Missing

| Capability | Status | Notes |
|---|---:|---|
| Upload datasets (CSV/Excel/JSON/Parquet) | ğŸŸ¡ | Upload exists, but end-to-end â€œuse in chat/analyticsâ€ is not complete. |
| Dataset profiling (types/missingness/outliers) | ğŸŸ¡ | Engines exist; needs automatic run + persistence + UI wiring. |
| â€œTalk to your dataâ€ autopilot story | âŒ | Needs narrative + evidence panels + suggested actions. |
| Grounded chat (tool-driven) | ğŸŸ¡ | Chat exists, but grounding tools are not consistently enforced. |
| SQL/NL2SQL for dataset | ğŸŸ¡ | NL2SQL engine exists; needs schema + execution + safety. |
| Analytics endpoints (forecasting, segmentation, etc.) | ğŸ§© | Some endpoints operate on mock data; need real dataset loading and results persistence. |
| Data quality and adequacy validation | ğŸŸ¡ | Validator exists; needs unified file storage + dataset linkage + auth. |
| Visualization generation | ğŸŸ¡ | Modules exist; needs deterministic spec + UI rendering + evidence links. |
| Report generation/export | ğŸŸ¡ | Report generator exists; needs integration with analyses and UI download. |
| Connectors to databases | ğŸŸ¡ | Connector scaffolding exists; needs secret handling, RBAC, UI workflows. |
| Multi-user auth + RBAC | ğŸŸ¡ | Auth scaffolding exists; needs durable user store, sessions, security hardening. |
| Job queue for heavy compute | âŒ | Required for reliability with many concurrent users. |
| Reproducibility (â€œanalysis as recipeâ€) | âŒ | Needs a first-class analysis plan and artifact model. |

---

## 4) What â€œComplete AI Data Analyst / Scientistâ€ means (scope)

To â€œcover everythingâ€ for a modern AI data analyst/scientist assistant, the product should at minimum support:

### 4.1 Analyst core (required)
- Schema and profile understanding
- Fast summaries and slice/dice
- Trends, correlations, and drivers
- Outlier/anomaly discovery
- Data quality diagnostics and cleaning plans
- Visualization and storytelling
- Exportable reports

### 4.2 Data scientist core (required)
- Task inference (classification/regression/forecasting/clustering)
- Target suggestion with confidence + required user confirmation when ambiguous
- Baseline modeling with correct splits and metrics
- Explainability (global + local where feasible)
- Leakage and bias checks
- Reproducible artifacts and model registry

### 4.3 Advanced (optional but strongly differentiating)
- Cohorts, funnels, retention, RFM, CLV
- Experiment analysis and lift measurement
- Causal inference modules as an opt-in workflow with strong disclaimers
- Text analytics and embeddings-based clustering
- Geospatial workflows

This repo already contains many engine modules in these categories; the main work is integration, safety, and UX.

---

## 5) Top-Level Roadmap (Milestones)

### Milestone A (P0): Make â€œTalk to your dataâ€ truthful and end-to-end
- Real dataset loading by dataset ID everywhere
- Compute layer with safe operators (SQL + statistics + plotting)
- Tool-driven chat with strict grounding
- â€œData Speaksâ€ autopilot page + evidence panels
- Contract alignment across UI and API

### Milestone B (P1): Make it delightful
- â€œInsight libraryâ€ (ranked insights with evidence)
- Transformations with preview and reproducibility
- Robust reporting and exports
- Integrated adequacy validation (with real uploads)

### Milestone C (P2): Make it a real â€œAI data scientistâ€
- Target/feature inference engine
- AutoML pipelines with leakage checks
- Model registry and prediction workflows
- Time series workflows

### Milestone D (P3): Production-grade reliability
- Job queue + workers
- Object storage abstraction for uploads and artifacts
- Multi-tenancy and quotas (optional for OSS, required for SaaS)
- Strong observability + SLOs

---

## 6) How to use the rest of this documentation pack

If your goal is to â€œbuild the best button that mesmerizes usersâ€, start here:
- Read `docs/TALK_TO_YOUR_DATA_FEATURE_SPEC.md` and implement the â€œData Speaksâ€ screen and workflow first.
- Then use `docs/TALK_TO_YOUR_DATA_ENGINEERING_SPEC.md` to implement the compute layer and tool contracts.
- Use `docs/TALK_TO_YOUR_DATA_SECURITY_EVAL_SPEC.md` to ensure the system is safe and provably grounded.

---

## 7) Does the current app meet the â€œAI Data Analyst / AI Data Scientistâ€ bar?

### 7.1 The honest answer
Structurally, it is close; functionally, not yet.

You already have:
- a UI that looks like an analyst assistant,
- a backend scaffold with dataset upload, storage, and many â€œanalysis enginesâ€,
- and a validator app that hints at a deeper â€œreadinessâ€ framework.

What you do not yet have (and what determines whether the product â€œworksâ€):
- a single end-to-end â€œtruth loopâ€ that turns real datasets into computed evidence artifacts,
- strict enforcement that all answers come from that evidence,
- and durable session + artifact persistence so the experience feels like a workflow, not a chat demo.

### 7.2 What â€œcompleteâ€ means (non-negotiable capability model)
AI Data Analyst Assistant (core) must reliably do:
- Load and understand any reasonable dataset (CSV/Excel/JSON/Parquet; mixed types; messy headers).
- Produce a trustworthy profile: shape, types, missingness, uniqueness, distributions, anomalies.
- Discover insights: segments, drivers, trends, outliers, relationships.
- Explain â€œwhy it mattersâ€ with evidence artifacts (tables, metrics, charts) that match the dataset.
- Offer next actions and prompts that are specific to the dataset (not generic).
- Export reports with reproducibility metadata.

AI Data Scientist Assistant (core) must reliably do:
- Infer task type when the user says â€œpredict/forecast/classify/clusterâ€ and ask for minimal clarifications.
- Suggest target variable candidates with confidence, show rationale, and require confirmation when ambiguous.
- Run baseline models with correct split strategies and metrics.
- Detect leakage risks, label leakage candidates, and propose safe feature sets.
- Provide explainability thatâ€™s honest and aligned with the chosen model and data type.
- Save model runs as artifacts (inputs, params, metrics, seed, dataset version).

The â€œmesmerizingâ€ button requires both, but in a staged UX:
- Stage 1: Analyst wow (Data Speaks autopilot story + evidence).
- Stage 2: Analyst chat (interactive slice/dice and explanations).
- Stage 3: Scientist workflows (modeling, explainability, prediction).

### 7.3 Where you are today vs. the capability model
The repo contains many engines that could power these features, but orchestration currently does not enforce grounding. In practice, today:
- The chat route responds without executing compute tools.
- The orchestrator tools return placeholder outputs.
- Some analytics endpoints run against generated demo data.
- Dataset upload exists, but stable user identity is not wired, causing â€œmissing datasetsâ€ from the userâ€™s POV.

This is why the product can look â€œcompleteâ€ while still failing the real promise when a user uploads their own data and expects truthful results.

---

## 8) The flagship experience (â€œMake the data talkâ€) â€” what it must do end-to-end

### 8.1 The Data Speaks pipeline (minimum)
When the user clicks the button for a dataset, the system should run a deterministic pipeline that produces a stable set of artifacts, and then generates the narrative from those artifacts:

1. Ingest and validate
   - confirm format + encoding + size limits
   - sanitize column names
   - infer stable column identifiers (so renames donâ€™t break artifacts)

2. Profile
   - compute schema and type inference
   - compute missingness and uniqueness
   - compute distributions and quantiles for numeric fields
   - compute value counts and top categories for categoricals
   - compute basic text stats (length, language hints) for text fields

3. Role inference
   - identify candidate keys/IDs
   - identify candidate time columns
   - identify candidate targets (for later modeling)
   - detect PII and sensitive columns

4. Quality + risk scan
   - duplicates and potential keys
   - invalid values and format inconsistencies
   - outliers and heavy tails
   - leakage indicators (post-outcome columns)
   - representation risks (rare segments)

5. Insight generation
   - generate candidate insights (many)
   - rank them by usefulness and confidence
   - keep only top K for the â€œwowâ€ story

6. Narrate with grounding
   - write a short story summarizing the top insights
   - attach the evidence artifacts (tables/charts)
   - include â€œwhat to do nextâ€ actions

7. Persist and index
   - store the session, plan, steps, and artifacts
   - allow re-run and export

### 8.2 What the user sees (minimum)
- A narrative header (top 3 takeaways + confidence + dataset scope).
- Evidence panels with overview, schema, quality, outliers, relationships, segments, time (if applicable), risk.
- Suggested next actions and prompts that reference their actual columns/entities.

---

## 9) The â€œdynamic codeâ€ requirement (how to satisfy it without unsafe execution)

### 9.1 Reframe the requirement
You want the system to behave like an expert analyst who reads the dataset, decides whatâ€™s important, chooses the right variables/targets, and produces the right computations and visualizations.

The key is: â€œdynamicâ€ should mean dynamic planning, not arbitrary code execution.

### 9.2 The recommended approach
Use a constrained â€œPlan â†’ Execute â†’ Explainâ€ loop:
- Plan: LLM generates a structured analysis plan from user intent + dataset profile.
- Execute: a deterministic compute engine executes the plan using an allow-list of operators.
- Explain: LLM narrates and proposes next steps, but only from artifacts.

This approach can handle unknown schemas and â€œany datasetâ€ while remaining safe, reproducible, and scalable.

---

## 10) Success criteria (what â€œdoneâ€ means for the flagship button)

### 10.1 User-visible acceptance criteria
The product meets the â€œTalk to your dataâ€ promise when a user can:
- Upload a dataset and immediately see the correct row/column counts and schema.
- Click â€œMake the data speakâ€ and get a narrative that is consistent with computed evidence.
- Ask follow-ups (counts, group-bys, trends) and always receive computed outputs.
- Export a report and re-run the session later with the same dataset version.

### 10.2 Engineering acceptance criteria (non-negotiables)
- A single dataset loader is used across routes, agents, and analytics endpoints.
- Every numeric claim is traceable to an artifact created by the compute engine.
- Heavy operations run as background jobs with progress + cancellation.
- Artifact storage is abstracted so deployments can switch from local disk to object storage.

### 10.3 Quality acceptance criteria (initial targets)
These can be adjusted, but you need explicit targets to prevent â€œdemo driftâ€:
- Grounding rate for numeric claims: 95%+ (production mode).
- Time-to-first-evidence for Data Speaks on a small dataset: under 10 seconds on a laptop.
- Time-to-first-evidence for a medium dataset: under 30 seconds with sampling-first.
- Injection success rate on adversarial suite: 0% for high-risk actions.

---

## 11) What to do next (implementation order)

This is the order that maximizes visible progress and prevents wasted work:

1. Implement the compute + artifact loop for core EDA operators (not all analytics).
2. Wire Data Speaks to use only computed artifacts.
3. Enforce grounding in chat (block numbers without evidence).
4. Replace mock dataset usage in analytics routes with real dataset loading.
5. Add durable user identity and dataset access control.
6. Integrate quality/adequacy as a first-class â€œreadinessâ€ feature tied to datasets.

The detailed task breakdown and mapping to current files is in:
- `docs/TALK_TO_YOUR_DATA_BACKLOG.md`

---

## 12) Glossary (shared language)

- **Artifact**: a stored output of computation (table, chart, report, metric) with provenance.
- **Dataset version**: an immutable fingerprint of dataset content (hash + metadata) used to ensure reproducibility.
- **Grounding**: the guarantee that answers come from computed artifacts rather than unverified model output.
- **Operator**: a safe, allow-listed compute primitive (profile, group-by, correlation, outlier scan, plot).
- **Plan**: a structured description of operators to run, with inputs/outputs and safety constraints.
- **Session (Data Speaks)**: the stored workflow instance created when the user clicks â€œMake the data speakâ€.
